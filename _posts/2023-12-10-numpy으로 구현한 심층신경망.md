---
layout: post
title:  소프트맥스 회귀를 이용한 mnist 이미지 분류
date: 2020-01-02 20:00:00 +0900
categories: AI
use_math: true
---
## 1. 프로젝트 소개
### 1) 주제 선정 이유
이전 프로젝트로 소프트맥스 회귀와 로지스틱 회귀를 알아보고, 다중분류와 이진분류 프로젝트를 해보았습니다. 하지만 은닉층이 없어 tensorflow 라이브러리처럼 층을 쌓을 수 없다는 것이 아쉬웠습니다. 은닉층을 함께 공부해보고 이전에 공부했던 것을 합쳐서 tensorflow 라이브러리처럼 만들어 더 깊게 인공지능을 이해해 보고 싶어 이 주제를 선정하였습니다.    

### 2) 프로젝트 소개
```python
# 직접 구현 라이브러리
model = Neural_Network(input=784, output=10, hidden=(512, 256))
model.fit(x_train, y_train, epoch=5, learning_rate=1, batch_size=600)
```
```
W1: (784, 512), b1: (1, 512)
W2: (512, 256), b2: (1, 256)
W3: (256, 10), b3: (10,)
Epoch:     1/5, loss: 13.027035, accuarcy: 0.109150 - 5.406672s
Epoch:     2/5, loss: 3.726099, accuarcy: 0.614083 - 5.472967s
Epoch:     3/5, loss: 0.836601, accuarcy: 0.851000 - 5.306957s
Epoch:     4/5, loss: 0.583201, accuarcy: 0.882933 - 5.173099s
Epoch:     5/5, loss: 0.485036, accuarcy: 0.897317 - 5.469006s
```
```python
model.evaluate(x_test, y_test)
```
```
(0.5066179317552106, 0.8895000000000001)
```
<br>
```python
# tensorflow 라이브러리
model = tf.keras.models.Sequential([
    tf.keras.layers.Flatten(input_shape=(28, 28)),
    tf.keras.layers.Dense(512, activation=tf.nn.sigmoid),
    tf.keras.layers.Dense(256, activation=tf.nn.sigmoid),
    tf.keras.layers.Dense(10, activation=tf.nn.softmax)
])
model.compile(optimizer='SGD',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
history = model.fit(x_train, y_train, epochs=5)
```
```
Epoch 1/5
1875/1875 [==============================] - 10s 5ms/step - loss: 2.1094 - accuracy: 0.4077
Epoch 2/5
1875/1875 [==============================] - 8s 4ms/step - loss: 1.3547 - accuracy: 0.7161
Epoch 3/5
1875/1875 [==============================] - 9s 5ms/step - loss: 0.7944 - accuracy: 0.8143
Epoch 4/5
1875/1875 [==============================] - 9s 5ms/step - loss: 0.5907 - accuracy: 0.8488
Epoch 5/5
1875/1875 [==============================] - 9s 5ms/step - loss: 0.4957 - accuracy: 0.8686
```
```python
model.evaluate(x_test, y_test)
```
```
313/313 [==============================] - 1s 3ms/step - loss: 0.4456 - accuracy: 0.8810
[0.44562962651252747, 0.8809999823570251]
```
학습데이터는 mnist 데이터를 이용하였습니다. 첫번째 사진이 직접 Neural_Network Class를 만들어서 학습시킨 사진입니다. 은닉층을 포함하여 input, output을 구현하였고 그 사이에 생기는 W, b들의 가중치들이 자동으로 학습되게 하였습니다. 두번째 사진은 실제 python에서 지원하는 tensorflow로 학습시킨 사진입니다. 직접 만든 Neural_Network Class와 거의 동일한 것을 볼 수 있습니다. 

## 2. 프로젝트 진행 과정
### 1) 오차역전파의 일반화
라이브러리를 제작하기 전, 은닉층이 있을 때 W, b들의 가중치들이 어떻게 업데이트 되는지 알아야 했습니다. 때문에 은닉층이 있을 경우까지의 구조를 그려보고 직접 미분하여 은닉층이 있을 때 어떻게 오차역전파가 수행되는지 구했습니다.<br>
![](/public/img/2023-12-10-numpy/Group%2026.png){: width="60%"}<br>
위에 사진처럼 이러한 형태의 구조가 있다고 생각해보았습니다. 2개의 x들이 입력으로 들어오고 하나의 은닉층, 시그모이드 함수를 거치고, 다시 softmax 함수로 들어가 3개의 y로 예측되는 형태입니다.  <br><br>

![](/public/img/2023-12-10-numpy/Group%206.png){: width="60%"}<br>
안에 자세한 수식들은 최종적으로 위 사진처럼 정리할 수 있습니다. 입력층, 은닉층, 각각의 값들이 다 계산된 값들입니다.  <br><br>

![](/public/img/2023-12-10-numpy/Group%2030.png){: width="60%"}<br>
![](/public/img/2023-12-10-numpy/render.png){: width="30%"}<br>
![](/public/img/2023-12-10-numpy/render%20(1).png){: width="30%"}<br>
먼저 출력층의 $W_{L}$와 $b_{L}$부터 가중치를 업데이트 해보겠습니다. 그러기 위해서는 전체 오차($E_{total}$)에 대해 $W_{L}$ 와 $b_{L}$를 미분한 값을 찾아야 합니다. 연쇄법칙의 의해 위 사진처럼 나타낼 수 있습니다. <br><br>

![](/public/img/2023-12-10-numpy/Group%209.png){: width="30%"}<br>
![](/public/img/2023-12-10-numpy/Group%208%20(4).png){: width="30%"}<br>
![](/public/img/2023-12-10-numpy/Group%207%20(2).png){: width="30%"}<br>
출력층의 $W_{L1}, W_{L2}, W_{L3}$ 차례대로 미분하여 각각의 미분한 값들을 구했습니다. <br><br>

![](/public/img/2023-12-10-numpy/Group%2029.png){: width="60%"}<br>
![](/public/img/2023-12-10-numpy/render%20(2).png){: width="30%"}<br>
![](/public/img/2023-12-10-numpy/render%20(3).png){: width="30%"}<br>
이제 은닉층의 $W_{H}$와 $b_{H}$의 가중치를 수정해보겠습니다. 이것도 똑같이 전체 오차($E_{total}$)에 대해 $W_{H}$와 $b_{H}$를 미분한 값을 찾으면 됩니다. <br><br>

![](/public/img/2023-12-10-numpy/render%20(4).png){: width="30%"}<br>
하지만 여기서 전체 오차($E_{total}$) 에 대한 $O_{H}$ 미분한 값은 어떻게 구할 수 있을까요? 그건 이전에 출력층에서 계산한 식을 이용하여 계산할 수 있습니다. <br><br>

![](/public/img/2023-12-10-numpy/render%20(5).png){: width="10%"}<br>
$W_{L}$와 $b_{L}$을 미분한 값에서 위 사진 부분이 $W_{H}$와 $b_{H}$을 미분할 때 반복되어진다는 것을 알 수 있습니다. (맨 처음 구조에서의 Loss_2의 부분입니다) <br><br>

![](/public/img/2023-12-10-numpy/Group%2010.png){: width="60%"}<br>
![](/public/img/2023-12-10-numpy/Group%2011%20(2).png){: width="60%"}<br>
![](/public/img/2023-12-10-numpy/Group%2012%20(1).png){: width="60%"}<br>
다음과 같이 풀어서 정리할 수 있습니다. <br><br>

![](/public/img/2023-12-10-numpy/Group%2020.png){: width="60%"}<br>
각각을 계산한 식을 이용하여 최종적으로 위 사진처럼 정리할 수 있습니다. <br><br>

![](/public/img/2023-12-10-numpy/Group%2021%20(1).png){: width="60%"}<br>
![](/public/img/2023-12-10-numpy/Group%2022%20(1).png){: width="60%"}<br>
최종적으로 정리한 식을 위 그림처럼 일반화할 수 있습니다. 

### 2) 은닉층 구현
이제 계산한 결과가 맞는지 확인해보기 위해 코드를 작성했습니다.<br>
```python
WH = np.random.randn(784, 512)
bH = np.zeros((1, 512))

WL = np.random.randn(512, 10)
bL = np.zeros(10)
```
먼저 $W_{H}$, $b_{H}$, $W_{L}$, $b_{L}$의 가중치들을 랜덤으로 생성해주었습니다.<br><br>
```python
epoch = 1000
learning_rate = 0.1
m = len(x_train)

for i in range(1, epoch+1):
    iH = np.dot(X, WH)+bH
    OH = sigmoid(iH)
    iL = np.dot(OH, WL)+bL
    OL = softmax(iL)

    Error = cross_entropy(Y, OL)
    Accuarcy = softmax_accuracy(Y, OL)
    print(f'Epoch: {i:5}/{epoch}, loss: {Error.item():f}, accuracy: {Accuarcy.item():f}')

    loss2 = OL-Y

    dWL = (1/m) * np.dot(OH.T, loss2)
    WL = WL - learning_rate * dWL
    dbL = (1/m) * np.sum(loss2, axis=0)
    bL = bL - learning_rate * dbL

    loss1 = OH * (1 - OH) * np.dot(loss2, WL.T)

    dWH = (1/m) * np.dot(X.T, loss1)
    WH = WH - learing_rate * dWH
    dbH = (1/m) * np.sum(loss, axis=0)
    bH = bH - learing_rate * dbH
```
그리고 아까 위에서 일반한식을 그대로 구현했습니다. 오차를 측정할 때에는 마지막 출력층이 softmax함수를 사용했기 때문에 cross entropy를 이용하였고, 정확도는 예측 한 값(가장 확률이 높은)이 얼마나 맞는지를 구해주었습니다. <br><br>
```
Epoch:     1/1000, loss: 13.183797, accuarcy: 0.076783
Epoch:     2/1000, loss: 12.230462, accuarcy: 0.082367
...
Epoch:   999/1000, loss: 0.858556, accuarcy: 0.849683
Epoch:  1000/1000, loss: 0.858122, accuarcy: 0.849800
```
학습을 1000번 정도 진행시켜 보니 loss가 0.8 아래로 떨어지는 것을 확인 할 수 있었고, 정확도는 85%정도로 나오는 것을 알 수 있었습니다. 학습이 잘 진행되는 걸로 봤을 때 위에서 구한 식이 문제가 없었고, 은닉층이 생겼을 때에도 가중치(W, b)들을 업데이트 할 수 있다는 것을 알 수 있었습니다.

### 3) 신경망 구현
이제 위에서 일반화 과정을 가지고 은닉층이 많아도, 없어도, 자동으로 가중치가 수정되도록 코드를 작성했습니다. 
```python
class Neural_Network:
    def __init__(self, input, output, hidden=None) -> None:
        self.input = input
        self.output = output
        self.hidden = hidden

        self.W = []
        self.b = []

        if self.hidden != None:
            self.W.append(np.random.randn(input, hidden[0]))
            self.b.append(np.zeros((1, hidden[0])))

            for i in range(len(hidden)-1):
                self.W.append(np.random.randn(self.hidden[i], self.hidden[i+1]))
                self.b.append(np.zeros((1, self.hidden[i+1])))

            self.W.append(np.random.randn(hidden[-1], output))
            self.b.append(np.zeros(output))
        else:
            self.W.append(np.random.randn(input, output))
            self.b.append(np.zeros(output))

        for i in range(len(self.W)):
            print(f'W{i+1}: {self.W[i].shape}, b{i+1}: {self.b[i].shape}')
```
<br>
```python
model = Neural_Network(input=784, output=10)
```
```
W1: (784, 10), b1: (10,)
```
<br>
```python
model = Neural_Network(input=784, output=10, hidden(512,))
```
```
W1: (784, 512), b1: (1, 512)
W2: (512, 10), b2: (10,)
```
먼저 가중치들이 자동으로 랜덤 생성되게 만들었습니다. 은닉층이 없을 때를 기본으로 생각하고, input과 output의 크기로 가중치(W, b)들을 랜덤 생성하고 모든 W, b를 저장하는 리스트에 넣도록 만들었습니다. 그리고 은닉층이 생겼을 때에도 똑같이 가중치(W, b)들을 랜덤으로 생성하고 리스트에 넣도록 만들었습니다. <br><br>

```python
def build_layer(self, X):
    self.A = []
    self.A.append(X) # A의 첫번째 데이터는 X
    for i in range(len(self.W)):

        if i+1 == len(self.W):
            if self.output == 1:
                self.A.append(self.sigmoid(np.dot(self.A[i], self.W[i])+self.b[i]))
            else:
                self.A.append(self.softmax(np.dot(self.A[i], self.W[i])+self.b[i]))
        else:
            self.A.append(self.sigmoid(np.dot(self.A[i], self.W[i])+self.b[i])) 
```
<br>
```python
model = Neural_Network(input=784, output=10, hidden(512,))
model.build_layer(x_train)
```
```
W1: (784, 512), b1: (1, 512)
W2: (512, 10), b2: (10,)
0
sigmoid
1
softmax
```
<br>
```python
model = Neural_Network(input=784, output=1, hidden(512,))
model.build_layer(x_train)
```
```
W1: (784, 512), b1: (1, 512)
W2: (512, 1), b2: (1,)
0
sigmoid
1
sigmoid
```
각 W, b들의 가중치를 업데이트하기 위해서 사용되는 오차역전파 규칙을 위해 필요한 것은 각 층의 출력들만 필요합니다. 그래서 x와 W, b들의 가중치을 가지고 각 층의 출력들이 자동으로 빌드되도록 만들었습니다. 여기서 output의 사이즈가 1이면 무조건 이진분류, output사이즈가 1이 아니면 다중분류라고 생각하고 그 조건만 잘 설정해주었습니다. <br><br>

```python
self.build_layer(X_batch)

# --------------------------------
# build_layer한 A를 이용하여 가중치 업데이트

self.loss.append(self.A[-1]-Y_batch)

for _ in reversed(range(1, len(self.A)-1)):
    dW = (1/batch_size) * np.dot(self.A[_].T, self.loss[-1])
    self.W[_] = self.W[_] - learning_rate * dW

    db = (1/batch_size) * np.sum(self.loss[-1], axis=0)
    self.b[_] = self.b[_] - learning_rate * db

    loss = self.A[_] * (1 - self.A[_]) * np.dot(self.loss[-1], self.W[_].T)
    self.loss.append(loss)

dW = (1/batch_size) * np.dot(self.A[0].T, self.loss[-1])
self.W[0] = self.W[0] - learning_rate * dW

db = (1/batch_size) * np.sum(self.loss[-1], axis=0)
self.b[0] = self.b[0] - learning_rate * db

# ----------------------------------
```
이제 가중치 부분이 학습되어야 합니다. 은닉층이 있을 때 일반화한 과정을 코드로 그대로 구현했습니다. 빌드한 A 리스트와 loss를 계산하는데로 loss 리스트 넣어서 거꾸로 가져오면서 가중치가 업데이트 되도록 코드를 작성했습니다. (이전 프로젝트에서 진행한 미니배치 경사하강법을 적용하여 더 빠르게 가중치가 학습되도록 하였습니다) <br><br>

```python
model = Neural_Network(input=784, output=10, hidden=(512, 256))
model.fit(x_train, y_train, epoch=5, learning_rate=1, batch_size=600)
```
```
W1: (784, 512), b1: (1, 512)
W2: (512, 256), b2: (1, 256)
W3: (256, 10), b3: (10,)
Epoch:     1/5, loss: 13.027035, accuarcy: 0.109150 - 5.406672s
Epoch:     2/5, loss: 3.726099, accuarcy: 0.614083 - 5.472967s
Epoch:     3/5, loss: 0.836601, accuarcy: 0.851000 - 5.306957s
Epoch:     4/5, loss: 0.583201, accuarcy: 0.882933 - 5.173099s
Epoch:     5/5, loss: 0.485036, accuarcy: 0.897317 - 5.469006s
```
그리고 코드를 돌려보면 은닉층이 있더라도 똑같이 오차역전파, 경사하강법이 적용되면서 loss가 줄어들고 가중치들이 학습되어 정확도가 오르는 것을 볼 수 있었습니다.

## 3. 느낀점
사실 프로젝트를 진행하면서 각 과정을 다 미분할 필요는 없었습니다. 하지만 가중치를 랜덤으로 생성하는 코드에서 np.random.rand와 np.random.randn의 차이로 인해 가중치가 잘 학습되지 않았고, 그로 인해 문제가 발생했었습니다. 나중에 찾아보니 표준정규분포와 가우시안 표준정규분포로 난수를 랜덤으로 생성하는 차이였고, 가중치를 랜덤으로 생성하는 과정도 굉장히 중요하다고 느꼈습니다. 시간이 된다면 이 부분에 대해서도 더 자세히 공부해 보고 싶고, 평소에 사용하던 tensorflow 라이브러리 만큼이나 성능을 내는 라이브러리를 직접 만들어 봐서 굉장히 뿌듯했습니다. 인공지능을 더 자세히 이해할 수 있어 좋았고, 진짜 다시 생각해봐도 인공지능의 구조를 만든 수학자들과 그 구조를 직접 구현하는 공학자들이 천재적이라는 생각이 들었습니다.

## 4. 참고문헌
- <https://towardsdatascience.com/part-2-gradient-descent-and-backpropagation-bf90932c066a>
- <https://nittaku.tistory.com/443>
- <https://jonathanweisberg.org/post/A Neural Network from Scratch - Part 1/>
- <https://neuralthreads.medium.com/backpropagation-made-super-easy-for-you-part-2-7b2a06f25f3c>
- <https://bubble-dev.tistory.com/entry/AI-Back-propagation오차역전파>
- <https://bubble-dev.tistory.com/entry/딥러닝-MNIST-필기체-인식>

## 5. GitHub
- <https://github.com/yc54616/model>