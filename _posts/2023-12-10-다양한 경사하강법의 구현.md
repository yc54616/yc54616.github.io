---
layout: post
title:  다양한 경사하강법의 구현
date: 2023-12-10 19:00:00 +0900
categories: AI
---
## 1. 프로젝트 소개
### 1) 주제 선정 이유
`로지스틱 회귀를 활용한 OX 이미지 판별`와 `소프트맥스 회귀를 이용한 minst 분류` 라는 프로젝트를 진행하면서 데이터를 학습 시킬 때 시행하는 경사하강법이 꽤 많은 시간이 걸렸습니다. 시간을 조금 더 단축시키고 싶어 조사를 통해 다양한 경사하강법이 있다는 것을 알게 되었고, 각각의 경사하강법의 차이를 이해하고 학습시간을 단축시키고 싶어 이 주제를 선정하였습니다.

### 2) 프로젝트 소개
![](/public/img/2023-12-10-many/Group%203%20(3).png){: width="60%" height="60%"}<br>
![](/public/img/2023-12-10-many/Group%202%20(5).png){: width="60%" height="60%"}<br>
![](/public/img/2023-12-10-many/Group%204%20(4).png){: width="60%" height="60%"}<br>
![](/public/img/2023-12-10-many/Group%205%20(1).png){: width="60%" height="60%"}<br>

일반적인 경사하강법, 배치 경사하강법, 확률적 경사하강법, 미니배치 경사하강법을 구현하였고, 각각 learning rate와 epoch 수를 적절히 조절하며 학습 데이터셋의 정확도를 약 90%로 맞추려고 하였습니다. 

## 2. 프로젝트 진행 과정
### 1) 경사하강법 (Gradient Descent)
가장 기본적인 경사하강법입니다. 배치 크기 1인 데이터셋을 epoch 수 만큼 돌면서 가중치들을 학습시킵니다. <br><br>
![](/public/img/2023-12-10-many/Group%203%20(1).png){: width="40%" height="40%"}<br>
![](/public/img/2023-12-10-many/Group%204%20(1).png){: width="30%" height="30%"}<br><br>

소프트맥스 회귀를 이용한 minst 분류 정리한 것처럼 각 W, b를 미분한 값을 활용하여 경사하강법을 시행하여 가중치를 학습시킬 수 있습니다.<br><br>

```python
for dataset_num in range(m):
    y_pred_train = softmax(np.dot(x_train, W)+b)
    train_loss = cross_entropy(y_train, y_pred_train)
    train_loss_hist.append(train_loss)
    train_acc = softmax_accuracy(y_train, y_pred_train)
    train_acc_hist.append(train_acc)

    y_pred_test = softmax(np.dot(x_test, W)+b)
    test_loss = cross_entropy(y_test, y_pred_test)
    test_loss_hist.append(test_loss)
    test_acc = softmax_accuracy(y_test, y_pred_test)
    test_acc_hist.append(test_acc)

    for i in range(1, epoch+1):
        x_train_one = x_train[dataset_num].reshape(1,-1)
        y_train_one = y_train[dataset_num].reshape(1,-1)

        y_pred_one = softmax(np.dot(x_train_one, W)+b)

        dW = np.dot(x_train_one.T, (y_pred_one-y_train_one))
        W = W - learning_rate * dW

        db = y_pred_one-y_train_one # 1개
        b = b - learning_rate * db
```
데이터셋(m)에서 하나씩 데이터를 가져와 epoch 수만큼 학습하도록 코딩하였습니다.

### 2) 배치 경사하강법 (Batch Gradient Descent)
행렬을 이용하여 배치 크기가 1이 아니라도 한번에 계산하여 경사하강법을 수행할 수 있습니다. <br><br>
![](/public/img/2023-12-10-many/Frame%207.png){: width="40%" height="40%"}<br>
![](/public/img/2023-12-10-many/Group%208%20(3).png){: width="30%" height="30%"}<br><br>

위 그림처럼 배치 크기가 4일 때(데이터셋이 4개)에는 각각을 계산하여 나온 미분값의 평균으로 각 W, b를 한번에 업데이트합니다. 때문에 일반적인 경사하강법의 비해 세밀하게 W, b들이 조절되어지지 않지만 계산하는 횟수가 훨씬 줄어 빠르게 가중치를 업데이트 할 수 있습니다. <br><br>

```python
for i in range(1, epoch+1):
    y_pred_train = softmax(np.dot(x_train, W)+b)
    train_loss = cross_entropy(y_train, y_pred_train)
    train_loss_hist.append(train_loss)
    train_acc = softmax_accuracy(y_train, y_pred_train)
    train_acc_hist.append(train_acc)

    y_pred_test = softmax(np.dot(x_test, W)+b)
    test_loss = cross_entropy(y_test, y_pred_test)
    test_loss_hist.append(test_loss)
    test_acc = softmax_accuracy(y_test, y_pred_test)
    test_acc_hist.append(test_acc)

    dW = (1/m) * np.dot(x_train.T, (y_pred_train-y_train))
    W = W - learning_rate * dW

    db = (1/m) * np.sum(y_pred_train-y_train, axis=0)
    b = b - learning_rate * db
```
행렬을 이용하여 한번에 미분하고 미분한 값의 평균을 내서 가중치들을 한번에 업데이트합니다. 

### 3) 확률적 경사 하강법 (Stochastic Gradient Descent)
확률적 경사하강법이란 무작위로 배치 크기가 1인 단 한 개의 데이터를 추출하여 기울기를 계산하고, 경사 하강법을 적용하는 방법입니다. 위에서의 경사하강법, 배치 경사하강법은 데이터셋 전체를 가지고 계산되어 전체 데이터셋에 대해 가중치 W, b들이 조절되었다면 확률형 경사하강법은 하나의 데이터에 대해 경사하강법을 시행하여 아주 빠르게 최적의 가중치들을 찾을 수 있습니다. <br><br>

![](/public/img/2023-12-10-many/Group%2017.png){: width="40%" height="40%"}<br>

```python
for i in range(1, epoch+1):
    y_pred_train = softmax(np.dot(x_train, W)+b)
    train_loss = cross_entropy(y_train, y_pred_train)
    train_loss_hist.append(train_loss)
    train_acc = softmax_accuracy(y_train, y_pred_train)
    train_acc_hist.append(train_acc)

    y_pred_test = softmax(np.dot(x_test, W)+b)
    test_loss = cross_entropy(y_test, y_pred_test)
    test_loss_hist.append(test_loss)
    test_acc = softmax_accuracy(y_test, y_pred_test)
    test_acc_hist.append(test_acc)

    for dataset_num in range(m):
        x_train_one = x_train[dataset_num].reshape(1,-1)
        y_train_one = y_train[dataset_num].reshape(1,-1)

        y_pred_one = softmax(np.dot(x_train_one, W)+b)

        dW = np.dot(x_train_one.T, (y_pred_one-y_train_one))
        W = W - learning_rate * dW

        db = y_pred_one-y_train_one # 1개
        b = b - learning_rate * db
```
데이터셋을 하나씩을 기준으로 가중치를 업데이트하고 이 과정을 epoch 수 만큼 학습되게 하였습니다.

### 4) 미니 배치 경사 하강법 (Mini-Batch Gradient Descent)
미니 배치 경사하강법은 배치 경사하강법과 확률적 경사하강법이 합쳐진 형태입니다. 전체 데이터를 미니 배치 형태로 조금씩 나누고 미니 배치 형태의 데이터셋을 확률적 경사하강법을 시행합니다. 전체 데이터가 아니라 미니 배치의 데이터이기 때문에 더 빠르게 최적의 가중치를 찾을 수 있고, 확률적 경사하강법보다 정밀하게 전체적인 가중치들의 값들을 조절할 수 있습니다.<br><br>

![](/public/img/2023-12-10-many/Group%2015%20(1).png){: width="40%" height="40%"}<br>
```python
for i in range(1, epoch+1):
    y_pred_train = softmax(np.dot(x_train, W)+b)
    train_loss = cross_entropy(y_train, y_pred_train)
    train_loss_hist.append(train_loss)
    train_acc = softmax_accuracy(y_train, y_pred_train)
    train_acc_hist.append(train_acc)

    y_pred_test = softmax(np.dot(x_test, W)+b)
    test_loss = cross_entropy(y_test, y_pred_test)
    test_loss_hist.append(test_loss)
    test_acc = softmax_accuracy(y_test, y_pred_test)
    test_acc_hist.append(test_acc)

    for batch_size_i in range(m//batch_size):
        start_i = batch_size_i*batch_size
        end_i = batch_size_i*batch_size+batch_size

        x_train_batch = x_train[start_i:end_i,]
        y_train_batch = y_train[start_i:end_i,]

        y_pred_batch = softmax(np.dot(x_train_batch, W)+b)

        dW = (1/batch_size) * np.dot(x_train_batch.T, (y_pred_batch-y_train_batch))
        W = W - learning_rate * dW

        db = (1/batch_size) * np.sum(y_pred_batch-y_train_batch, axis=0)
        b = b - learning_rate * db
```
확률적 경사하강법에 배치를 적용시킨 코드입니다. 확률적 경사하강법에서는 1개의 데이터셋으로 학습을 돌렸다면 미니배치 경사하강법에서는 배치 크기만큼 데이터셋을 선택하여 확률적 경사하강법으로 학습시킵니다.

## 3. 느낀점
사실 처음 `소프트맥스 회귀를 이용한 minst 분류` 프로젝트를 진행했을 때 코드가 데이터셋이 한개였을 때에는 잘 작동했는데 배치 사이즈, 데이터셋이 여러개일 경우에는 잘 작동하지 않았습니다. 나중에 보니 배치 사이즈가 있을 때에는 평균으로 계산해준다는 것을 알게 되었습니다. 이러한 시행착오를 통해 더 다양한 경사하강법을 알게 될 수 있어 좋았고, 실제보다 거의 10배는 빠르게 학습이 진행된 것 같아 너무 뿌듯했습니다. 그동안 고민하던 문제를 해결할 수 있어 좋았고, 진짜 인공지능을 깊게 공부할 수록 이러한 구조를 만든 수학자들과 공학자들이 정말 대단한 것 같았습니다.

## 4. 참고문헌
- <https://velog.io/@arittung/DeepLearningStudyDay8>
- <https://velog.io/@cha-suyeon/혼공머-배치와-미니-배치-확률적-경사하강법>
- <https://skyil.tistory.com/68>

## 5. GitHub
- <https://github.com/yc54616/gradient-descent>