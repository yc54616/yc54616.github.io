---
layout: post
title:  소프트맥스 회귀를 이용한 mnist 이미지 분류
categories: AI
---
## 1. 프로젝트 소개
### 1) 주제 선정 이유
2학년 겨울방학 때 `<인공지능 기초 다지기>` 코칭스터디라는 활동을 했었습니다. 5주동안 1주일에 정해진 강의를 듣고 과제를 하는 온라인 스터디였습니다. 코칭 스터디를 하면서 인공지능의 사용되는 백터, 행렬, 파이썬 기초 등을 배웠습니다. 그리고 인공지능 수학 탐구 보고서를 쓰면서 선형회귀, 경사하강법을 주제로 더 확장해서 공부해봤습니다. 또한 경사하강법을 확장해서 로지스틱 회귀라는 알고리즘을 가지고 OX 이미지 판별 프로젝트를 알고리즘 보고서를 쓰며 진행했었습니다. 이제 최종적으로 소프트맥스 회귀를 공부하며 다중분류에 대해 이해하고 전반적인 분류 인공지능에 대해 이해하고 싶어 이 주제를 선정하였습니다.

### 2) 프로젝트 소개
![](/public/img/2023-12-10-softmax/image01.png){: width="30%" height="30%"}<br>
(그림 1)<br>
python keras에서 지원해주는 대표적인 인공지능 데이터셋 MNIST 손글씨 숫자를 활용하여 진행한 프로젝트입니다. 학습데이터를 학습하여 소프트맥스 함수을 거쳐 나온 가장 큰 확률의 인덱스를 찾아 정답을 예측하여 맞추는 프로젝트입니다. 정확도는 학습 데이터 86%, 테스트 데이터 83%가 나왔습니다.

## 2. 프로젝트 진행 과정
### 1) 소프트맥스 함수(Softmax Function)
![](/public/img/2023-12-10-softmax/image04.png){: width="30%" height="30%"}<br>
(그림 2)<br>
![](/public/img/2023-12-10-softmax/image05.png){: width="30%" height="30%"}<br>
(그림 3)<br>
소프트맥스 함수란 입력받은 값을 출력으로 0~1사이의 값으로 모두 정규화하며 출력 값들의 총합은 항상 1이 되는 특성을 가진 함수입니다. 로지스틱 함수를 일반화한 함수이고 다중분류에 사용합니다. (그림 2)와 같은 수식으로 이루어져 있습니다.

### 2) 원핫 인코딩(One-Hot-Encoding)
![](/public/img/2023-12-10-softmax/image06.png){: width="30%" height="30%"}<br>
(그림 4)<br>
원핫 인코딩이란 데이터를 수많은 0과 한개의 1의 값으로 데이터를 구별하는 인코딩입니다. 다중분류 문제에서 정답데이터의 형태로 사용합니다.

### 3) 교차 엔트로피 오차(Cross Entropy Error, CEE)
![](/public/img/2023-12-10-softmax/image07.png){: width="30%" height="30%"}<br>
(그림 5)<br>
교차 엔트로피 오차는 다중분류 문제의 정답 데이터와 학습 데이터의 오차를 구하기 위해 사용됩니다. n는 정답데이터의 개수, 는 정답데이터의 인덱스 i의 값, 는 가설함수의 인덱스 i의 값을 나타냅니다.

### 4) 소프트맥스 회귀
![](/public/img/2023-12-10-softmax/image08.png){: width="30%" height="30%"}<br>
(그림 6)<br>
소프트맥스 회귀는 다중분류 문제를 해결하기 위해 사용합니다. (그림 6)와 같이 꽃받침 길이, 꽃받침 넓이, 꽃잎 길이, 꽃잎 넓이의 feature들을 활용하여 꽃의 종류를 맞추는 문제를 해결할 때 소프트맥스 회귀를 사용합니다.<br><br>

![](/public/img/2023-12-10-softmax/image09.png){: width="60%" height="60%"}<br>
(그림 7)<br>
소프트맥스 회귀를 사용하여 문제를 해결하는 과정을 나타내면 (그림 7)과 같습니다.<br><br>

> 1 `|` x1, x2, x3, x4의 feature들을 입력받습니다. <br>
> 2 `|` W1 ~ W12와 b1 ~ b3를 이용하여 4개의 x(데이터의 feature)를 3개의 특성(정답 개수)로 차원을 축소합니다. 차원을 축소할 때에는 (그림 7)의 z1, z2, z3와 같이 W와 x가 곱해지고 더해지며 차원이 축소됩니다. <br>
> 3 `|` 소프트맥스 함수를 거쳐서 z1, z2, z3의 값들의 간격은 그대로, 0~1 사이의 정규화됩니다. <br>
> 4 `|` 소프트맥스 함수를 거친 z1, z2, z3(a1, a2, a3)와 원핫 인코딩 된 정답데이터 y1, y2, y3를 비교하여 오차를 측정합니다. <br>
> 5 `|` 오차 함수(교차 엔트로피)를 이용하여 오차를 측정한 뒤 경사하강법을 이용하여 가중치들을 업데이트합니다. <br>
> 6 `|` 4, 5를 계속 반복하며 가중치들을 학습합니다. <br>
> 7 `|` 학습된 가중치와 softmax함수를 거쳐 나온 값들 중 가장 높은 값(확률)을 가진 값이 정답이 됩니다. <br>

<br>※ 경사하강법은 이전 프로젝트에서 진행하여 따로 설명하지 않고 넘어가겠습니다. <br>
- <https://yc54616.github.io/ai/2023/12/10/%EB%A1%9C%EC%A7%80%EC%8A%A4%ED%8B%B1-%ED%9A%8C%EA%B7%80%EB%A5%BC-%ED%99%9C%EC%9A%A9%ED%95%9C-OX-%EC%9D%B4%EB%AF%B8%EC%A7%80-%ED%8C%90%EB%B3%84.html>

### 5) 소프트맥스 회귀 과정 미분
경사하강법을 하기 위해서는 LOSS, LOSS를 각 W로 미분한 값들이 필요합니다. 이를 위해 (그림 7)의 LOSS를 미분하는 과정을 정리해보았습니다.<br><br>
![](/public/img/2023-12-10-softmax/image10.png){: width="30%" height="30%"}<br>
(그림 8)<br>
![](/public/img/2023-12-10-softmax/image11.png){: width="30%" height="30%"}<br>
(그림 9)<br>
![](/public/img/2023-12-10-softmax/image12.png){: width="30%" height="30%"}<br>
(그림 10)<br><br>
![](/public/img/2023-12-10-softmax/image13.png){: width="30%" height="30%"}<br>
(그림 11)<br>
![](/public/img/2023-12-10-softmax/image14.png){: width="30%" height="30%"}<br>
(그림 12)<br>
![](/public/img/2023-12-10-softmax/image15.png){: width="30%" height="30%"}<br>
(그림 13)<br><br>
![](/public/img/2023-12-10-softmax/image16.png){: width="30%" height="30%"}<br>
(그림 14)<br>
(그림 7)의 식을 위 (그림 8) ~ (그림 14)와 같이 각각 정리하였습니다. 그럼 먼저 W1으로 LOSS를 미분해보겠습니다.<br><br>
![](/public/img/2023-12-10-softmax/image17.png){: width="30%" height="30%"}<br>
(그림 15)<br><br>
![](/public/img/2023-12-10-softmax/image18.png){: width="30%" height="30%"}<br>
(그림 16)<br><br>
![](/public/img/2023-12-10-softmax/image19.png){: width="30%" height="30%"}<br>
![](/public/img/2023-12-10-softmax/image20.png){: width="30%" height="30%"}<br>
(그림 17)<br><br>
![](/public/img/2023-12-10-softmax/image21.png){: width="30%" height="30%"}<br>
(그림 18)<br><br>
![](/public/img/2023-12-10-softmax/image22.png){: width="30%" height="30%"}<br>
(그림 19)<br>
(그림 15)에 첫 번째 부분부터 미분해보겠습니다. 각각은 미분하면 (그림 16) ~ (그림 18)이 되고 최종적으로 (그림 19)이 되는 것을 알 수 있습니다.<br><br>

![](/public/img/2023-12-10-softmax/image23.png){: width="30%" height="30%"}<br>
(그림 20)<br><br>
![](/public/img/2023-12-10-softmax/image24.png){: width="30%" height="30%"}<br>
(그림 21)<br><br>
![](/public/img/2023-12-10-softmax/image25.png){: width="30%" height="30%"}<br>
(그림 22)<br><br>
![](/public/img/2023-12-10-softmax/image26.png){: width="30%" height="30%"}<br>
(그림 23)<br>
(그림 15)에 두 번째 부분도 미분해보겠습니다. 각각은 미분하면 (그림 20) ~ (그림 22)이 되고 최종적으로 (그림 23)이 되는 것을 알 수 있습니다.<br><br>

![](/public/img/2023-12-10-softmax/image27.png){: width="30%" height="30%"}<br>
(그림 24)<br><br>
![](/public/img/2023-12-10-softmax/image28.png){: width="30%" height="30%"}<br>
(그림 25)<br><br>
![](/public/img/2023-12-10-softmax/image29.png){: width="30%" height="30%"}<br>
(그림 26)<br><br>
![](/public/img/2023-12-10-softmax/image30.png){: width="30%" height="30%"}<br>
(그림 27)<br>
마지막으로 (그림 15)에 세 번째 부분을 미분해보겠습니다. 각각은 미분하면 (그림 24) ~ (그림 26)이 되고 최종적으로 (그림 27)이 되는 것을 알 수 있습니다.<br><br>

![](/public/img/2023-12-10-softmax/image43.png){: width="30%" height="30%"}<br>
![](/public/img/2023-12-10-softmax/image44.png){: width="30%" height="30%"}<br>
![](/public/img/2023-12-10-softmax/image45.png){: width="30%" height="30%"}<br>
![](/public/img/2023-12-10-softmax/image46.png){: width="30%" height="30%"}<br>
![](/public/img/2023-12-10-softmax/image47.png){: width="30%" height="30%"}<br>
(그림 28)<br><br>

![](/public/img/2023-12-10-softmax/image31.png){: width="30%" height="30%"}<br>
![](/public/img/2023-12-10-softmax/image32.png){: width="30%" height="30%"}<br>
![](/public/img/2023-12-10-softmax/image33.png){: width="30%" height="30%"}<br>
![](/public/img/2023-12-10-softmax/image34.png){: width="30%" height="30%"}<br>
![](/public/img/2023-12-10-softmax/image35.png){: width="30%" height="30%"}<br>
![](/public/img/2023-12-10-softmax/image36.png){: width="30%" height="30%"}<br>
![](/public/img/2023-12-10-softmax/image37.png){: width="30%" height="30%"}<br>
![](/public/img/2023-12-10-softmax/image38.png){: width="30%" height="30%"}<br>
![](/public/img/2023-12-10-softmax/image39.png){: width="30%" height="30%"}<br>
![](/public/img/2023-12-10-softmax/image40.png){: width="30%" height="30%"}<br>
![](/public/img/2023-12-10-softmax/image41.png){: width="30%" height="30%"}<br>
![](/public/img/2023-12-10-softmax/image42.png){: width="30%" height="30%"}<br>
(그림 29)<br><br>

![](/public/img/2023-12-10-softmax/image48.png){: width="30%" height="30%"}<br>
![](/public/img/2023-12-10-softmax/image49.png){: width="30%" height="30%"}<br>
![](/public/img/2023-12-10-softmax/image50.png){: width="30%" height="30%"}<br>
(그림 30)<br><br>

![](/public/img/2023-12-10-softmax/image51.png){: width="50%" height="50%"}<br>
(그림 31)<br>
![](/public/img/2023-12-10-softmax/image52.png){: width="30%" height="30%"}<br>
(그림 32)<br>
(그림 28)의 규칙으로 각 W의 대해 미분한 값을 구해보면 (그림 29), (그림 30)이 되고 행렬과 백터를 이용해 한번에 (그림 31), (그림 32)으로 나타낼 수 있습니다.

### 6) 확률적 경사하강법(SGD)
확률적 경사하강법이란 학습 데이터셋에서 무작위로 한 개의 샘플 데이터셋을 추출해 그 샘플에 대해서만 기울기를 계산하여 업데이트하는 방법입니다. 확률적 경사하강법은 샘플 데이터를 이용하기 때문에 다뤄야 할 데이터가 줄어들고, 학습 속도가 빠르다는 장점을 가지고 있습니다. 하지만 진폭이 크고 경사 하강법보다 불안정하게 움직이기 때문에 아주 정확한 최적의 해를 찾기 어려울 수 있습니다. (그림 33)의 경사하강법과 달리 확률적 경사하강법은 (그림 34)와 같이 학습됩니다.<br>
![](/public/img/2023-12-10-softmax/image53.png){: width="30%" height="30%"}<br>
(그림 33)<br>
![](/public/img/2023-12-10-softmax/image54.png){: width="30%" height="30%"}<br>
(그림 34)<br>

### 7) mnist 이미지 분류 시나리오
소프트맥스 회귀 이론을 바탕으로 mnist 이미지 분류 시나리오를 생각해보았습니다. <br><br>

> 1 `|` python keras mnist 데이터셋을 로드합니다. <br>
> 2 `|` 이미지를 (28, 28) -> (1, 784) 로 쭉 늘립니다. <br>
> 3 `|` 학습데이터를 확률적 경사하강법을 이용하여 가중치을 학습합니다. <br>
> 4 `|` 저장된 가중치를 활용하여 테스트 데이터를 테스트합니다. <br>

### 8) mnist 이미지 분류 구현
#### (1) keras mnist 데이터셋 로드
```python
from keras.datasets.mnist import load_data

(x_train, y_train), (x_test, y_test) = load_data(path='mnist.npz')
```
python keras datasets을 활용하여 mnist 데이터셋을 다운로드 후 학습 데이터와 테스트 데이터로 나눕니다.

#### (2) 이미지 전처리
```python
import numpy as np

def one_hot_encoding(x):
    tmp = np.zeros((len(x), 1, max(x)+1))
    for idx, data in enumerate(x):
        tmp[idx,0,data] = 1
    return tmp
    
x_train = x_train.reshape((len(x_train), 1, -1))
y_train = one_hot_encoding(y_train)
x_test = x_test.reshape((len(x_test), 1, -1))
y_test = one_hot_encoding(y_test)
x_train.shape, y_train.shape, x_test.shape, y_test.shape
```
```
((60000, 1, 784), (60000, 1, 10), (10000, 1, 784), (10000, 1, 10))
```
이미지 데이터는 (1, 784)의 형태로 바꿔주고 정답 데이터는 원핫 인코딩 형태로 만들어 주었습니다.<br><br>

```python
data = [[x,y] for x, y in zip(x_train, y_train)]
```
이미지 데이터와 학습데이터를 하나로 합쳐주었습니다.<br><br>

```python
import random
random.shuffle(data)
```
데이터를 랜덤 라이브러리를 활용하여 섞어 주었습니다.

#### (3) 가중치 학습
```python
def softmax(x):
    c = np.max(x)
    np_exp = np.exp(x - c)
    return np_exp / np.sum(np_exp)

W = np.random.rand(784,10)
b = np.random.rand(1,10)

epoch = 10000 # 학습 횟수
lr = 0.000001    # learning rate 설정
delta = 1e-7 # 오버플로우 방지

hist = []

loss = -1

for i in range(epoch):
    hist_sum = []
    for x_train, y_train in data:
        y_pred = x_train @ W + b
        loss = -np.sum(y_train*np.log(softmax(y_pred)+delta))+delta

        Y = softmax(y_pred)-y_train
        dW = x_train.T @ Y
        db = Y
        W = W - lr*dW
        b = b - lr*db

        hist_sum.append(loss)
        hist.append(loss)
    print(f'Epoch: {i:5}/{epoch}, loss: {np.sum(hist_sum)/len(data)}, total_loss: {np.sum(hist_sum)}')
```
```
Epoch:     0/10000, loss: 14.508594451322718, total_loss: 14508.594451322719
Epoch:     1/10000, loss: 14.039207526908744, total_loss: 14039.20752690874
...
Epoch:  9998/10000, loss: 7.892096217047862e-06, total_loss: 0.007892096217047862
Epoch:  9999/10000, loss: 7.891253355622722e-06, total_loss: 0.007891253355622721
```
확률적 경사하강법을 구현하여 모든 데이터를 한번씩 돌면서 기울기를 측정, 가중치 업데이트를 반복하였습니다.<br><br>

![](/public/img/2023-12-10-softmax/image60.png){: width="30%" height="30%"}<br>
위처럼 LOSS가 점점 떨어지는 걸 볼 수 있습니다.

#### (4) 정확도 테스트
```python
print('학습 데이터 정확도:', sum([np.argmax(softmax(x_train[idx] @ W + b)) == y_train[idx] for idx in range(len(x_train))])/len(x_train))
```
```
학습 데이터 정확도: 0.7435166666666667
```

```python
print('테스트 데이터 정확도:', sum([np.argmax(softmax(x_test[idx] @ W + b)) == y_test[idx] for idx in range(len(x_test))])/len(x_test))
```
```
테스트 데이터 정확도: 0.749
```
학습 데이터의 정확도는 74%, 테스트 데이터의 74%가 나왔습니다.

## 3. 느낀점
소프트맥스 회귀를 사용하여 mnist 이미지 데이터를 분류해보았습니다. full batch, 미니 배치, 확률적 경사하강법의 3개의 경사하강법을 구현하여 각각 학습해보았는데 이유는 잘 모르겠지만 full batch, 미니배치가 잘 학습되지 않았습니다. 은닉층이 없거나, 구현이 잘못된 거 같아서 조금 아쉽고 다음에 프로젝트를 더 보강해야겠다고 느꼈습니다. 평소프로젝트에는 인공지능 라이브러리를 사용하여 진행했었는데 이번에 더 정교하게 구현해보니 수학적 이론들이 공학에 적용되었을 때 엄청난 시너지를 만들어내는 것이 놀라웠습니다. 평소 일반 과목들을 조금 소홀히 했었는데 이를 통해 일반 과목 공부도 더 열심히 해야할 것 같습니다.

## 4. 참고자료
- <https://yhyun225.tistory.com/14>
- <https://kwonkai.tistory.com/32>
- <https://wikidocs.net/35476>
- <https://kibua20.tistory.com/213>
- <https://incomeplus.tistory.com/243>
- <https://sacko.tistory.com/19>
- <https://wikidocs.net/37406>

## 5. Github
- <https://github.com/yc54616/softmax_classification_mnist>